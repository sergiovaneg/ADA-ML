\documentclass{scrartcl}

\usepackage{graphicx}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{microtype}
\usepackage[inkscapeformat=eps, inkscapepath=svgdir]{svg}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{biblatex}
\usepackage{hyperref}
\usepackage{cleveref}

\addbibresource{main.bib}

\title{Advanced Data Analysis and Machine Learning - Practical Activities}
\subtitle{Transformer Architecture}
\author{Sergio Mauricio Vanegas Arias}
\date{\today}

\begin{document}

  \maketitle

  \section{Introduction}

    This week, we were asked to experiment with the Transformer architecture\cite{vaswani2023attention} to solve the same modelling task we selected last week for the \emph{Recursive Network Architectures}' assignment, and then compare their performance.

    As a personal note, I realized that I failed to include the predictions made by the recursive architectures in my input signal, which made the performance comparison to the ARIMAX model non-representative of their real capabilities. Therefore, this week I re-trained the models using the fed-back signal of interest as the only input for all the evaluated models. Furthermore, I included an additional metric called "short-term RMSE", which measures the forecast accuracy over the first $48$ hours after the end of the ground-truth or context provided to the model.

    The specific tasks were the following:
    \begin{itemize}
      \item Describe the model architecture.
      \item Set up the Transformer Model training to forecast solar energy production.
      \item Fine-tune the hyperparameters of the model using a grid search over 4 arbitrary (yet relevant) architectural properties.
      \item Compare the Transformer model's performance against that of the recursive networks.
    \end{itemize}

  \section{Architecture Description}

    \begin{figure}[ht]
      \centering
      \includesvg[width=\textwidth]{./diagrams/transformer.svg}
      \caption{Transformer architecture adapted to time-series}
      \label{fig:transformer}
    \end{figure}

    In \Cref{fig:transformer}\footnote{The colour-delineated blocks in the diagram are the ones whose hyperparameters where tuned through the grid search, with blocks of the same colour sharing parameters for a single configuration.}, the implemented architecture is shown. The main differences with respect to the original Transformer architecture are the lack of embedding (since the input vector already corresponds to numerical data instead of tokenized text) and the efficient implementation of the feed-forward network in the Encoder/Decoder subnetworks using a 1D Convolutional Layer with kernel size $1$.

    The context and input are of the shape $N_B \times L \times 1$, where $N_B$ is the batch size and $N_L$ is the length of the memory. This is then encoded into the shape \textit{Batch size} $N_B \times N_L \times N_d$, where $N_d$ is the depth of the model, also used for the size of each head in the Multi-Head-Attention (MHA) layers. The MHA layers, in turn have a number $N_H$ of independent heads.

    In the feed-forward 1D convolution layers, the traditional Dense Layer is replaced by a domain-specific convolutional layer with $N_{FF}$ operating on the output of the attention blocks. Finally, The output of the decoder is reduced to the original dimensionality of the input as recommended in \url{https://keras.io/examples/timeseries/timeseries_transformer_classification/}, and then passed through a Single-Layer Perceptron (SLP) of dimension $N_{SLP}$, the output of which is sent to a linear activation layer to recover the time-series prediction.

    The training is done in parallel by passing the same entries as both context and input, whereas the forecast is done by freezing the context in the last samples available for training and recursively updating the input with the predictions made by the model.

  \section{Grid Search}

    In order to optimize the hyperparameters of the Architecture, a grid search is performed over the following parameters and values:
    \begin{itemize}
      \item The input/context memory ($L$): $24/48/72$ hours/samples
      \item The head/encoding dimensionality ($N_d$): $2/4/8$
      \item The number of attention heads ($N_h$): $2/4/8$
      \item The number of convolutional filters ($N_{FF}$): $2/5/10$
      \item The number of SLP units ($N_{SLP}$): $8/16/32$
    \end{itemize}

    As a result, the optimal configuration was determined by comparing their ST-RMSE; this yielded the following configuration: 
    \begin{itemize}
      \item Input/context memory: 48 hours
      \item Head/encoding dimensionality: 2
      \item Number of attention heads: 4
      \item Number of convolutional filters: 10
      \item Number of SLP units: 16
    \end{itemize}
    This set of parameters resulted in a Short-Term RMSE of $0.423$, an RMSE of $1.377$, and a Maximum absolute error of $3.031$. The resulting forecast can be visualized in \Cref{fig:transformer_forecast}.

    \begin{figure}[ht]
      \centering
      \includesvg[width=\textwidth]{./figures/model_20231126175353_fit.svg}
      \caption{Optimal Transformer model forecast}
      \label{fig:transformer_forecast}
    \end{figure}

  \section{Model Comparison}

    As mentioned in the introduction, the performance of the optimal Transformer was compared against that of the updated recursive models, which were trained using the same input memory as the fine-tuned network of this week ($L$ samples), and using a number of units equal to $N_h + N_{FF} + N_{SLP}$ from the optimal setup. The resulting forecasts can be seen in \Cref{fig:rnn_forecast,fig:lstm_forecast} for the RNN and LSTM models respectively.

    \begin{figure}[ht]
      \centering
      \includesvg[width=\textwidth]{../../ML_week3/report/figures/rnn_fit.svg}
      \caption{Optimal RNN model forecast}
      \label{fig:rnn_forecast}
    \end{figure}

    \begin{figure}[ht]
      \centering
      \includesvg[width=\textwidth]{../../ML_week3/report/figures/lstm_fit.svg}
      \caption{Optimal LSTM model forecast}
      \label{fig:lstm_forecast}
    \end{figure}

    \begin{table}[ht]
      \centering
      \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Model} & \textbf{RMSE} & \textbf{Max AE} & \textbf{ST-RMSE} \\ \hline
        \textbf{Transformer} & $1.377$ & $3.031$ & $0.423$ \\ \hline
        \textbf{RNN} & $1.301$ & $3.654$ & $0.844$ \\ \hline
        \textbf{LSTM} & $1.349$ & $3.079$ & $1.628$ \\ \hline
      \end{tabular}
      \caption{Model performance comparison}
      \label{tb:comparison}
    \end{table}

    As it is evidenced by \Cref{tb:comparison}, the context preservation strategy of the Transformer model positively impacts its short-term forecast capabilities compared to last week's recursive models. In the long-term, however, all models try to preserve an extremely generic shape, failing to model predict the changes in the trend of the weather and sticking its seasonality.

  \printbibliography

\end{document}