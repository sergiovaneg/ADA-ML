\documentclass{scrartcl}
\usepackage[UKenglish]{babel}
\usepackage{graphicx}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{microtype}
\usepackage[inkscapeformat=eps, inkscapepath=svgdir]{svg}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{hyperref}

\usepackage{cleveref}

\title{Advanced Data Analysis and Machine Learning - Practical Activities}
\subtitle{Sequential Data}
\author{Sergio Mauricio Vanegas Arias}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}

  This week, we were asked to experiment with Recursive Neural Network architectures. Particularly, we were told to choose a type of dataset (``Time-Series forecasting'' in the case of this report) and compare the performance of the ML architectures with that of an Auto-Regressive (AR) model.

  The specific tasks are the following:

  \begin{enumerate}
    \item Import, pre-process, and visualize the dataset.
    \item Fit and compare:
    \begin{itemize}
      \item A baseline autoregressive model;
      \item A traditional RNN;
      \item An LSTM.
    \end{itemize}
    \item Interpret and comment on the results of each modelling strategy.
  \end{enumerate}

\section{Dataset Pre-Processing and Visualization}

  The chosen dataset was \href{https://www.kaggle.com/datasets/chaitanyakumar12/time-series-forecasting-of-solar-energy/}{\emph{Time series forecasting of solar energy}}, where the target variable is \emph{solar\_mw}. Before going any further, it is worth mentioning that the dataset contains a typo in row 2184 of the \emph{wind-direction} variable, where the authors inserted the text-string ``am'' instead of a numerical value. This instance was removed by hand and later interpolated.

  Out of the box, without dropping bad entries or performing any kind of interpolation, the dataset was as in \Cref{fig:timeseries_raw}. It is easy to notice that only the last fraction of the dataset contains any data beyond the target variable, so we drop all observations containing any missing value.

  \begin{figure}[ht]
    \centering
    \includesvg[width=0.8\textwidth]{figures/timeseries_raw.svg}
    \caption{Raw dataset}
    \label{fig:timeseries_raw}
  \end{figure}

  The result of this operation and interpolating linearly for any missing value within the remainder of the dataset is shown in \Cref{fig:timeseries_clean}. In this state, it is easy to visualize the periodicity and non-stationary character of the target variable, which merits the fitting of an ARIMAX model.

  \begin{figure}[ht]
    \centering
    \includesvg[width=0.8\textwidth]{figures/timeseries_clean.svg}
    \caption{Clean dataset}
    \label{fig:timeseries_clean}
  \end{figure}

  Finally, for the sake of numerical standardization (and since the focus of this report is comparing model accuracy instead of the application itself), we normalize all variables using the \emph{z-score} method. The normalized dataset is shown in \Cref{fig:timeseries_norm}

  \begin{figure}[ht]
    \centering
    \includesvg[width=0.8\textwidth]{figures/timeseries_norm.svg}
    \caption{Normalized dataset}
    \label{fig:timeseries_norm}
  \end{figure}

  The initial $70\%$ of the normalized dataset was used for training, whereas the remaining $30\%$ was reserved for validation.

\section{Baseline Model}

  The first fitted model, as mentioned earlier, was an Auto-Regressive Moving-Average with Exogenous input (ARIMAX) model, the order of which was determined by iterating over the orders of the autoregressive, differences, and moving average components ($p$, $d$, and $q$ respectively) and comparing the prediction Root-Mean-Squared Error (RMSE) over the validation signal\footnote{After the grid search was performed, the optimal parameters were fixed in the code to reduce execution time.}. The optimal model ended up being an $ARIMAX(5,0,3)$ model (which is equivalent to an $ARMAX(5,3)$), with a forecast RMSE of $0.805$ and a Maximum Absolute Error (Max AE) of $1.847$. The graphical results can be observed in \Cref{fig:baseline_fit}.

  \begin{figure}[ht]
    \centering
    \includesvg[width=0.8\textwidth]{figures/baseline_fit.svg}
    \caption{Baseline model - Forecast}
    \label{fig:baseline_fit}
  \end{figure}

  It can be easily observed that, despite the good fit of the training dataset, the model is incapable of generalizing its dynamics to the test region. Thus, a more complex approach is required

\section{Recursive Models}

  The models in this section were created using the \emph{Tensorflow/Keras} framework, which boasts great computational performance while maintaining a relatively simple API.

  For the sake of a fair comparison, but also keeping in mind the increased complexity of the recursive architectures, we make use of thrice the optimal ARIMAX autoregressive degree $p$ to define the number of units in the recursive layer and thrice the moving-average degree $q$ to define the number of past inputs considered per estimation. We also limit the maximum number of training epochs to $1000$, but implements an early stop based on the validation ($20\%$ of the training dataset, $100$ epochs of patience) MSE.

  \subsection{Simple Recursive Neural Network}

    We first try using a traditional recursive architecture, fitting the data to a stateless RNN layer connected to a Linear Regression (Dense) layer. The statelessness of the model means that the output of each observation does not depend on the internal state of the network after estimating the previous output, which means that the training process can be parallelized; nevertheless, we get that at the expense of losing the state information every time we make a new estimation. 

    The model training yields the results in \Cref{fig:rnn_fit}, with a forecast RMSE of $1.212$ and a Max AE of $3.168$. Despite the numerically worse performance, we can observe that the forecasted signal trend is better tracked; this is mainly due to the validation split that avoids the overfitting observed for the ARMAX model.

    \begin{figure}[ht]
      \centering
      \includesvg[width=0.8\textwidth]{figures/rnn_fit.svg}
      \caption{Simple RNN model - Forecast}
      \label{fig:rnn_fit}
    \end{figure}

  \subsection{LSTM Neural Network}
    
    A more modern LSTM architecture is now used in place of the traditional RNN, otherwise keeping the same architecture and statelessness.

    The model training yields the results in \Cref{fig:lstm_fit}, with a forecast RMSE of $1.409$ and a MAE of $3.301$. Once again, the numerical drop in performance is due to the validation split.

    \begin{figure}[ht]
      \centering
      \includesvg[width=0.8\textwidth]{figures/lstm_fit.svg}
      \caption{LSTM model - Forecast}
      \label{fig:lstm_fit}
    \end{figure}

\section{Further Discussion}

  In both recursive architecture scenarios, we see an increase in squared error; but when we look at \Cref{fig:baseline_fit}, we see that the relatively low error comes from the model outputting an almost null signal that is, on average, closer to the real value without really trying to recover the dynamics of the solar power production.

  Moreover, using the same number of units and inputs, we can notice that the intrinsically more complex of the LSTM layer with respect to the Simple RNN one makes it perform worse.

  Some strategies tried to improve the model but not fully discussed in the report were:
  \begin{itemize}
    \item Using a stateful recursive network, which makes use of the internal state of the recursive layer for every subsequent estimation at the expense of lack of training parallelization.
    \item Using multiple recursive layers, which resulted in a decreased generalization ability and longer training times.
    \item Using a larger amount of input lags/recursive units, which only resulted in an early termination after the patience period.
  \end{itemize}

\end{document}